{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "Extracting the score of the match from a single frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HSA_OVERRIDE_GFX_VERSION=10.3.0\n"
     ]
    }
   ],
   "source": [
    "%env HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from util import vid_to_frames\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda:0')\n",
    "#     print('Using the GPU 😎')\n",
    "# else:\n",
    "#     device = torch.device('cpu')\n",
    "#     print('Using the CPU ‼️')\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L's score is 2 digits\n"
     ]
    }
   ],
   "source": [
    "test_vid = './test/test_vid.mp4'\n",
    "\n",
    "PATCH_HEIGHT = 14\n",
    "PATCH_WIDTH = 19\n",
    "\n",
    "PATCH_TOP = 310\n",
    "PATCH_BOTTOM = PATCH_TOP + PATCH_HEIGHT\n",
    "L_PATCH_LEFT = 268\n",
    "L_PATCH_RIGHT = L_PATCH_LEFT + PATCH_WIDTH\n",
    "L_PATCH_MID = (L_PATCH_LEFT + L_PATCH_RIGHT) // 2 - 2\n",
    "R_PATCH_LEFT = 357\n",
    "R_PATCH_RIGHT = R_PATCH_LEFT + PATCH_WIDTH\n",
    "R_PATCH_MID = (R_PATCH_LEFT + R_PATCH_RIGHT) // 2 - 2\n",
    "ADJ = -32\n",
    "\n",
    "cap = cv2.VideoCapture(test_vid)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 16_000)\n",
    "flags, arb_frame = cap.read()\n",
    "arb_frame = cv2.cvtColor(arb_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "lwhole_patch = arb_frame[PATCH_TOP:PATCH_BOTTOM, L_PATCH_LEFT:L_PATCH_RIGHT]\n",
    "_, lwhole_patch = cv2.threshold(lwhole_patch, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "lhalf_patch1 = lwhole_patch.copy()\n",
    "lhalf_patch1[:, (L_PATCH_MID-L_PATCH_LEFT):] = 0\n",
    "\n",
    "lhalf_patch2 = lwhole_patch.copy()\n",
    "lhalf_patch2[:, :(L_PATCH_MID-L_PATCH_LEFT)] = 0\n",
    "_, lhalf_patch2 = cv2.threshold(lhalf_patch2, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "hpad = np.zeros((lhalf_patch2.shape[0], 28 - lhalf_patch2.shape[1]))\n",
    "lhalf_patch2 = np.hstack((lhalf_patch2, hpad))\n",
    "\n",
    "cv2.imshow('whole',lwhole_patch)\n",
    "cv2.imshow('half1',lhalf_patch1)\n",
    "cv2.imshow('half2',lhalf_patch2)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if np.any(lwhole_patch[:, 0] > 100):\n",
    "    print(\"L's score is 2 digits\")\n",
    "else:\n",
    "    print(\"L's score is 1 digit\")\n",
    "\n",
    "rwhole_patch = arb_frame[PATCH_TOP:PATCH_BOTTOM, R_PATCH_LEFT:R_PATCH_RIGHT]\n",
    "_, rwhole_patch = cv2.threshold(rwhole_patch, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "rhalf_patch1 = rwhole_patch.copy()\n",
    "rhalf_patch1[:, (R_PATCH_MID-R_PATCH_LEFT):] = 0\n",
    "\n",
    "rhalf_patch2 = rwhole_patch.copy()\n",
    "rhalf_patch2[:, :(R_PATCH_MID-R_PATCH_LEFT)] = 0\n",
    "_, rhalf_patch2 = cv2.threshold(rhalf_patch2, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "cv2.imshow('whole',rwhole_patch)\n",
    "cv2.imshow('half1',rhalf_patch1)\n",
    "cv2.imshow('half2',rhalf_patch2)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bounding boxes (tlx, tly, brx, bry):\n",
    "\n",
    "height: 310-330\n",
    "width: 265-290, 355-375\n",
    "\n",
    "lscore: (265, 310, 290, 330)\n",
    "rscore: (355, 310, 370, 330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, SiglipForImageClassification\n",
    "\n",
    "model_name = \"prithivMLmods/Mnist-Digits-SigLIP2\"\n",
    "mnist_model = SiglipForImageClassification.from_pretrained(model_name)\n",
    "input_processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "PATCH_HEIGHT = 14\n",
    "PATCH_WIDTH = 19\n",
    "\n",
    "PATCH_TOP = 310\n",
    "PATCH_BOTTOM = PATCH_TOP + PATCH_HEIGHT\n",
    "L_PATCH_LEFT = 268\n",
    "L_PATCH_RIGHT = L_PATCH_LEFT + PATCH_WIDTH\n",
    "L_PATCH_MID = (L_PATCH_LEFT + L_PATCH_RIGHT) // 2 - 2\n",
    "R_PATCH_LEFT = 358\n",
    "R_PATCH_RIGHT = R_PATCH_LEFT + PATCH_WIDTH\n",
    "R_PATCH_MID = (R_PATCH_LEFT + R_PATCH_RIGHT) // 2 - 2\n",
    "ADJ = -32\n",
    "\n",
    "def predict_score_from_frame(frame, view_patches=False):\n",
    "    blurred = frame\n",
    "    # blurred = cv2.blur(frame, (2, 2))\n",
    "\n",
    "    def vert_pad(patch):\n",
    "        vpad = np.zeros((28 - patch.shape[0], patch.shape[1]))\n",
    "        return np.vstack((patch, vpad))\n",
    "\n",
    "    def horiz_pad(patch):\n",
    "        hpad = np.zeros((patch.shape[0], 28 - patch.shape[1]))\n",
    "        return np.hstack((patch, hpad))\n",
    "\n",
    "    # the whole patch of FotL's score\n",
    "    # this will be most accurate if score is 1 digit\n",
    "    lwhole_patch = blurred[PATCH_TOP:PATCH_BOTTOM, L_PATCH_LEFT:L_PATCH_RIGHT]\n",
    "    _, lwhole_patch = cv2.threshold(lwhole_patch, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "    l_double = np.any(lwhole_patch[:, 0] > 100) # if any pixels on the left border are lit, it's double digits (1X where 0 <= X <= 5)\n",
    "    lwhole_patch = horiz_pad(lwhole_patch)\n",
    "    # lwhole_patch = vert_pad(lwhole_patch)\n",
    "\n",
    "    # the masked RHS half-patch of FotL's score\n",
    "    lhalf_patch2 = lwhole_patch.copy()\n",
    "    lhalf_patch2[:, :(L_PATCH_MID-L_PATCH_LEFT)] = 0\n",
    "    _, lhalf_patch2 = cv2.threshold(lhalf_patch2, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "    lhalf_patch2 = horiz_pad(lhalf_patch2)\n",
    "    # lhalf_patch2 = vert_pad(lhalf_patch2)\n",
    "\n",
    "    if view_patches:\n",
    "        cv2.imshow('left patch whole', lwhole_patch)\n",
    "        cv2.imshow('left patch half2', lhalf_patch2)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # the whole patch of FotR's score\n",
    "    rwhole_patch = blurred[PATCH_TOP:PATCH_BOTTOM, R_PATCH_LEFT:R_PATCH_RIGHT]\n",
    "    _, rwhole_patch = cv2.threshold(rwhole_patch, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "    r_double = np.any(rwhole_patch[:, 0] > 100) # if any pixels on the left border are lit, it's double digits (1X where 0 <= X <= 5)\n",
    "    rwhole_patch = horiz_pad(rwhole_patch)\n",
    "    # rwhole_patch = vert_pad(rwhole_patch)\n",
    "\n",
    "    # the masked RHS half-patch of FotR's score\n",
    "    rhalf_patch2 = rwhole_patch.copy()\n",
    "    rhalf_patch2[:, :(L_PATCH_MID-L_PATCH_LEFT)] = 0\n",
    "    _, rhalf_patch2 = cv2.threshold(rhalf_patch2, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "    rhalf_patch2 = horiz_pad(rhalf_patch2)\n",
    "    # rhalf_patch2 = vert_pad(rhalf_patch2)\n",
    "\n",
    "    if view_patches:\n",
    "        cv2.imshow('right patch whole', rwhole_patch)\n",
    "        cv2.imshow('left patch half2', rhalf_patch2)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    lwhole_input = input_processor(images=Image.fromarray(lwhole_patch), return_tensors='pt')\n",
    "    lhalf2_input = input_processor(images=Image.fromarray(lhalf_patch2), return_tensors='pt')\n",
    "\n",
    "    rwhole_input = input_processor(images=Image.fromarray(rwhole_patch), return_tensors='pt')\n",
    "    rhalf2_input = input_processor(images=Image.fromarray(rhalf_patch2), return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lwhole_outputs = mnist_model(**lwhole_input)\n",
    "        lwhole_logits = lwhole_outputs.logits\n",
    "        lwhole_probs = torch.nn.functional.softmax(lwhole_logits, dim=1).squeeze()\n",
    "\n",
    "        lhalf2_outputs = mnist_model(**lhalf2_input)\n",
    "        lhalf2_logits = lhalf2_outputs.logits\n",
    "        lhalf2_probs = torch.nn.functional.softmax(lhalf2_logits, dim=1).squeeze()\n",
    "\n",
    "        rwhole_outputs = mnist_model(**rwhole_input)\n",
    "        rwhole_logits = rwhole_outputs.logits\n",
    "        rwhole_probs = torch.nn.functional.softmax(rwhole_logits, dim=1).squeeze()\n",
    "\n",
    "        rhalf2_outputs = mnist_model(**rhalf2_input)\n",
    "        rhalf2_logits = rhalf2_outputs.logits\n",
    "        rhalf2_probs = torch.nn.functional.softmax(rhalf2_logits, dim=1).squeeze()\n",
    "\n",
    "    def most_likely(whole_probs, half2_probs, double_digit):\n",
    "        if double_digit:\n",
    "            num = 10 + half2_probs.argmax().item()\n",
    "            prob = half2_probs.max().item()\n",
    "\n",
    "            # 17 not a possible answer, but 1's get classified as 7's too often so this is most likely\n",
    "            if num == 17:\n",
    "                num = 11\n",
    "        else:\n",
    "            num = whole_probs.argmax().item()\n",
    "            prob = whole_probs.max().item()\n",
    "\n",
    "        return num, prob\n",
    "\n",
    "    return most_likely(lwhole_probs, lhalf2_probs, l_double),\\\n",
    "            most_likely(rwhole_probs, rhalf2_probs, r_double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, 16_500)\n",
    "# flags, arbitrary_frame = cap.read()\n",
    "# arbitrary_frame = cv2.cvtColor(arbitrary_frame, cv2.COLOR_BGR2GRAY)\n",
    "# print(predict_score_from_frame(arbitrary_frame, view_patches=True))\n",
    "\n",
    "# cv2.imshow('arbitrary frame', arbitrary_frame)\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "for i in range(1, 84):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, i * 250)\n",
    "    flags, arbitrary_frame = cap.read()\n",
    "    arbitrary_frame = cv2.cvtColor(arbitrary_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    (lscore, lconf), (rscore, rconf) = predict_score_from_frame(arbitrary_frame)\n",
    "\n",
    "    cv2.imshow(f'frame {i * 500}: {lscore} to {rscore} ({lconf*100:.2f}%, {rconf*100:.2f}%)', arbitrary_frame)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling scores with MNIST model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m score_info = \u001b[43mpd\u001b[49m.DataFrame(columns=[\u001b[33m'\u001b[39m\u001b[33mms\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlscore\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrscore\u001b[39m\u001b[33m'\u001b[39m], index=[])\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_score_from_frame\u001b[39m(index, cap, score_info):\n\u001b[32m     11\u001b[39m     num_frames = \u001b[38;5;28mint\u001b[39m(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<stringsource>:69\u001b[39m, in \u001b[36mcfunc.to_py.__Pyx_CFunc_b0409f__29_pydevd_sys_monitoring_cython_object__lParen__etc_to_py_4code_4line.wrap\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1470\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1512\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._internal_line_event\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1313\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._stop_on_breakpoint\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m_pydevd_sys_monitoring\\\\_pydevd_sys_monitoring_cython.pyx:1950\u001b[39m, in \u001b[36m_pydevd_sys_monitoring_cython._do_wait_suspend\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/AI_ML/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2188\u001b[39m, in \u001b[36mPyDB.do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, exception_type)\u001b[39m\n\u001b[32m   2185\u001b[39m             from_this_thread.append(frame_custom_thread_id)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads_suspended_single_notification.notify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         keep_suspended = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace_suspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2190\u001b[39m frames_list = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[32m   2193\u001b[39m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/AI_ML/lib/python3.12/site-packages/debugpy/_vendored/pydevd/pydevd.py:2257\u001b[39m, in \u001b[36mPyDB._do_wait_suspend\u001b[39m\u001b[34m(self, thread, frame, event, arg, trace_suspend_type, from_this_thread, frames_tracker)\u001b[39m\n\u001b[32m   2254\u001b[39m                 queue.put(internal_cmd)\n\u001b[32m   2255\u001b[39m                 wait_timeout = TIMEOUT_FAST\n\u001b[32m-> \u001b[39m\u001b[32m2257\u001b[39m         \u001b[43mnotify_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2258\u001b[39m         notify_event.clear()\n\u001b[32m   2260\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/AI_ML/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/AI_ML/lib/python3.12/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#! python3\n",
    "\n",
    "import time\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "score_info = pd.DataFrame(columns=['ms', 'lscore', 'rscore'], index=[])\n",
    "\n",
    "def get_score_from_frame(index, cap, score_info):\n",
    "    num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if index >= num_frames:\n",
    "        raise Exception(f'index {index} out of bounds for {num_frames} frames in video')\n",
    "\n",
    "    if index not in score_info.index:\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, index)\n",
    "        flag, frame = cap.read()\n",
    "        time = cap.get(cv2.CAP_PROP_POS_MSEC)\n",
    "        cv2.imshow(f'Frame{index}', cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY))\n",
    "        cv2.waitKey()\n",
    "        score = [int(s) for s in input('enter the scores separated by a space: ').split()]\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        score_info.loc[index, 'ms'] = score[0]\n",
    "        score_info.loc[index, 'lscore'] = score[0]\n",
    "        score_info.loc[index, 'rscore'] = score[1]\n",
    "\n",
    "    return score_info.loc[index, 'lscore'], score_info.loc[index, 'rscore']\n",
    "\n",
    "cap = cv2.VideoCapture('./test/test_vid.mp4')\n",
    "num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "PATCH_WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "PATCH_WIDTH = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# will contain index of all frames where the score changes\n",
    "score_change_frames = [0]\n",
    "l = 0 # first frame\n",
    "r = num_frames - 1 # last frame\n",
    "\n",
    "# outer loop\n",
    "while get_score_from_frame(l, cap, score_info) != get_score_from_frame(r, cap, score_info):\n",
    "    t = get_score_from_frame(l, cap, score_info)\n",
    "    while l != r - 1:\n",
    "        m = (l + r) // 2\n",
    "        lscore = get_score_from_frame(l, cap, score_info)\n",
    "        mscore = get_score_from_frame(m, cap, score_info)\n",
    "        if lscore == mscore:\n",
    "            l = m\n",
    "        else:\n",
    "            r = m\n",
    "    score_change_frames.append(r)\n",
    "    l = r\n",
    "    r = num_frames - 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
