{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "Extracting the score of the match from a single frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env HSA_OVERRIDE_GFX_VERSION=10.3.0\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from util import vid_to_frames\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda:0')\n",
    "#     print('Using the GPU 😎')\n",
    "# else:\n",
    "#     device = torch.device('cpu')\n",
    "#     print('Using the CPU ‼️')\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolating score bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L's score is 2 digits\n"
     ]
    }
   ],
   "source": [
    "test_vid = './test/test_vid.mp4'\n",
    "\n",
    "PATCH_HEIGHT = 14\n",
    "PATCH_WIDTH = 19\n",
    "\n",
    "PATCH_TOP = 310\n",
    "PATCH_BOTTOM = PATCH_TOP + PATCH_HEIGHT\n",
    "L_PATCH_LEFT = 268\n",
    "L_PATCH_RIGHT = L_PATCH_LEFT + PATCH_WIDTH\n",
    "L_PATCH_MID = (L_PATCH_LEFT + L_PATCH_RIGHT) // 2 - 2\n",
    "R_PATCH_LEFT = 357\n",
    "R_PATCH_RIGHT = R_PATCH_LEFT + PATCH_WIDTH\n",
    "R_PATCH_MID = (R_PATCH_LEFT + R_PATCH_RIGHT) // 2 - 2\n",
    "ADJ = -32\n",
    "\n",
    "cap = cv2.VideoCapture(test_vid)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 16_000)\n",
    "flags, frame = cap.read()\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "lwhole_patch = frame[PATCH_TOP:PATCH_BOTTOM, L_PATCH_LEFT:L_PATCH_RIGHT]\n",
    "_, lwhole_patch = cv2.threshold(lwhole_patch, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "lhalf_patch1 = lwhole_patch.copy()\n",
    "lhalf_patch1[:, (L_PATCH_MID-L_PATCH_LEFT):] = 0\n",
    "\n",
    "lhalf_patch2 = lwhole_patch.copy()\n",
    "lhalf_patch2[:, :(L_PATCH_MID-L_PATCH_LEFT)] = 0\n",
    "_, lhalf_patch2 = cv2.threshold(lhalf_patch2, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "hpad = np.zeros((lhalf_patch2.shape[0], 28 - lhalf_patch2.shape[1]))\n",
    "lhalf_patch2 = np.hstack((lhalf_patch2, hpad))\n",
    "\n",
    "cv2.imshow('whole',lwhole_patch)\n",
    "cv2.imshow('half1',lhalf_patch1)\n",
    "cv2.imshow('half2',lhalf_patch2)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if np.any(lwhole_patch[:, 0] > 100):\n",
    "    print(\"L's score is 2 digits\")\n",
    "else:\n",
    "    print(\"L's score is 1 digit\")\n",
    "\n",
    "rwhole_patch = frame[PATCH_TOP:PATCH_BOTTOM, R_PATCH_LEFT:R_PATCH_RIGHT]\n",
    "_, rwhole_patch = cv2.threshold(rwhole_patch, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "rhalf_patch1 = rwhole_patch.copy()\n",
    "rhalf_patch1[:, (R_PATCH_MID-R_PATCH_LEFT):] = 0\n",
    "\n",
    "rhalf_patch2 = rwhole_patch.copy()\n",
    "rhalf_patch2[:, :(R_PATCH_MID-R_PATCH_LEFT)] = 0\n",
    "_, rhalf_patch2 = cv2.threshold(rhalf_patch2, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "cv2.imshow('whole',rwhole_patch)\n",
    "cv2.imshow('half1',rhalf_patch1)\n",
    "cv2.imshow('half2',rhalf_patch2)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bounding boxes (tlx, tly, brx, bry):\n",
    "\n",
    "height: 310-330\n",
    "width: 265-290, 355-375\n",
    "\n",
    "lscore: (265, 310, 290, 330)\n",
    "rscore: (355, 310, 370, 330)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, SiglipForImageClassification\n",
    "\n",
    "model_name = \"prithivMLmods/Mnist-Digits-SigLIP2\"\n",
    "mnist_model = SiglipForImageClassification.from_pretrained(model_name)\n",
    "input_processor = AutoImageProcessor.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "PATCH_HEIGHT = 14\n",
    "PATCH_WIDTH = 19\n",
    "\n",
    "PATCH_TOP = 310\n",
    "PATCH_BOTTOM = PATCH_TOP + PATCH_HEIGHT\n",
    "L_PATCH_LEFT = 268\n",
    "L_PATCH_RIGHT = L_PATCH_LEFT + PATCH_WIDTH\n",
    "L_PATCH_MID = (L_PATCH_LEFT + L_PATCH_RIGHT) // 2 - 2\n",
    "R_PATCH_LEFT = 358\n",
    "R_PATCH_RIGHT = R_PATCH_LEFT + PATCH_WIDTH\n",
    "R_PATCH_MID = (R_PATCH_LEFT + R_PATCH_RIGHT) // 2 - 2\n",
    "ADJ = -32\n",
    "\n",
    "def predict_score_from_frame(frame, view_patches=False):\n",
    "    blurred = frame\n",
    "    # blurred = cv2.blur(frame, (2, 2))\n",
    "\n",
    "    def vert_pad(patch):\n",
    "        vpad = np.zeros((28 - patch.shape[0], patch.shape[1]))\n",
    "        return np.vstack((patch, vpad))\n",
    "\n",
    "    def horiz_pad(patch):\n",
    "        hpad = np.zeros((patch.shape[0], 28 - patch.shape[1]))\n",
    "        return np.hstack((patch, hpad))\n",
    "\n",
    "    # the whole patch of FotL's score\n",
    "    # this will be most accurate if score is 1 digit\n",
    "    lwhole_patch = blurred[PATCH_TOP:PATCH_BOTTOM, L_PATCH_LEFT:L_PATCH_RIGHT]\n",
    "    _, lwhole_patch = cv2.threshold(lwhole_patch, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "    l_double = np.any(lwhole_patch[:, 0] > 100) # if any pixels on the left border are lit, it's double digits (1X where 0 <= X <= 5)\n",
    "    lwhole_patch = horiz_pad(lwhole_patch)\n",
    "    # lwhole_patch = vert_pad(lwhole_patch)\n",
    "\n",
    "    # the masked RHS half-patch of FotL's score\n",
    "    lhalf_patch2 = lwhole_patch.copy()\n",
    "    lhalf_patch2[:, :(L_PATCH_MID-L_PATCH_LEFT)] = 0\n",
    "    _, lhalf_patch2 = cv2.threshold(lhalf_patch2, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "    lhalf_patch2 = horiz_pad(lhalf_patch2)\n",
    "    # lhalf_patch2 = vert_pad(lhalf_patch2)\n",
    "\n",
    "    if view_patches:\n",
    "        cv2.imshow('left patch whole', lwhole_patch)\n",
    "        cv2.imshow('left patch half2', lhalf_patch2)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    # the whole patch of FotR's score\n",
    "    rwhole_patch = blurred[PATCH_TOP:PATCH_BOTTOM, R_PATCH_LEFT:R_PATCH_RIGHT]\n",
    "    _, rwhole_patch = cv2.threshold(rwhole_patch, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "    r_double = np.any(rwhole_patch[:, 0] > 100) # if any pixels on the left border are lit, it's double digits (1X where 0 <= X <= 5)\n",
    "    rwhole_patch = horiz_pad(rwhole_patch)\n",
    "    # rwhole_patch = vert_pad(rwhole_patch)\n",
    "\n",
    "    # the masked RHS half-patch of FotR's score\n",
    "    rhalf_patch2 = rwhole_patch.copy()\n",
    "    rhalf_patch2[:, :(L_PATCH_MID-L_PATCH_LEFT)] = 0\n",
    "    _, rhalf_patch2 = cv2.threshold(rhalf_patch2, 255 + ADJ, 255, cv2.THRESH_BINARY)\n",
    "    rhalf_patch2 = horiz_pad(rhalf_patch2)\n",
    "    # rhalf_patch2 = vert_pad(rhalf_patch2)\n",
    "\n",
    "    if view_patches:\n",
    "        cv2.imshow('right patch whole', rwhole_patch)\n",
    "        cv2.imshow('left patch half2', rhalf_patch2)\n",
    "        cv2.waitKey()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "    lwhole_input = input_processor(images=Image.fromarray(lwhole_patch), return_tensors='pt')\n",
    "    lhalf2_input = input_processor(images=Image.fromarray(lhalf_patch2), return_tensors='pt')\n",
    "\n",
    "    rwhole_input = input_processor(images=Image.fromarray(rwhole_patch), return_tensors='pt')\n",
    "    rhalf2_input = input_processor(images=Image.fromarray(rhalf_patch2), return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        lwhole_outputs = mnist_model(**lwhole_input)\n",
    "        lwhole_logits = lwhole_outputs.logits\n",
    "        lwhole_probs = torch.nn.functional.softmax(lwhole_logits, dim=1).squeeze()\n",
    "\n",
    "        lhalf2_outputs = mnist_model(**lhalf2_input)\n",
    "        lhalf2_logits = lhalf2_outputs.logits\n",
    "        lhalf2_probs = torch.nn.functional.softmax(lhalf2_logits, dim=1).squeeze()\n",
    "\n",
    "        rwhole_outputs = mnist_model(**rwhole_input)\n",
    "        rwhole_logits = rwhole_outputs.logits\n",
    "        rwhole_probs = torch.nn.functional.softmax(rwhole_logits, dim=1).squeeze()\n",
    "\n",
    "        rhalf2_outputs = mnist_model(**rhalf2_input)\n",
    "        rhalf2_logits = rhalf2_outputs.logits\n",
    "        rhalf2_probs = torch.nn.functional.softmax(rhalf2_logits, dim=1).squeeze()\n",
    "\n",
    "    def most_likely(whole_probs, half2_probs, double_digit):\n",
    "        if double_digit:\n",
    "            num = 10 + half2_probs.argmax().item()\n",
    "            prob = half2_probs.max().item()\n",
    "\n",
    "            # 17 not a possible answer, but 1's get classified as 7's too often so this is most likely\n",
    "            if num == 17:\n",
    "                num = 11\n",
    "        else:\n",
    "            num = whole_probs.argmax().item()\n",
    "            prob = whole_probs.max().item()\n",
    "\n",
    "        return num, prob\n",
    "\n",
    "    return most_likely(lwhole_probs, lhalf2_probs, l_double),\\\n",
    "            most_likely(rwhole_probs, rhalf2_probs, r_double)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, 16_500)\n",
    "# flags, arbitrary_frame = cap.read()\n",
    "# arbitrary_frame = cv2.cvtColor(arbitrary_frame, cv2.COLOR_BGR2GRAY)\n",
    "# print(predict_score_from_frame(arbitrary_frame, view_patches=True))\n",
    "\n",
    "# cv2.imshow('arbitrary frame', arbitrary_frame)\n",
    "# cv2.waitKey()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "for i in range(1, 84):\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, i * 250)\n",
    "    flags, arbitrary_frame = cap.read()\n",
    "    arbitrary_frame = cv2.cvtColor(arbitrary_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    (lscore, lconf), (rscore, rconf) = predict_score_from_frame(arbitrary_frame)\n",
    "\n",
    "    cv2.imshow(f'frame {i * 500}: {lscore} to {rscore} ({lconf*100:.2f}%, {rconf*100:.2f}%)', arbitrary_frame)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifying labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exceptional = 977, nominal = 15506, %exceptional = 5.93%, %nominal = 94.07%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csvs = os.listdir('./Data/ScoreInfo/Sabre/')\n",
    "\n",
    "nominal = 0\n",
    "exceptional = 0\n",
    "\n",
    "for csv in csvs:\n",
    "    info = pd.read_csv('./Data/ScoreInfo/Sabre/' + csv, header=0)\n",
    "\n",
    "    prev_lscore, prev_rscore = 0, 0\n",
    "    for i, row in info.iterrows():\n",
    "        lscore, rscore = row['lscore'], row['rscore']\n",
    "        if prev_lscore > lscore or prev_rscore > rscore:\n",
    "            exceptional += 1\n",
    "        elif lscore > prev_lscore + 1 or rscore > prev_rscore + 1:\n",
    "            exceptional += 1\n",
    "        else:\n",
    "            nominal += 1\n",
    "\n",
    "        prev_lscore, prev_rscore = lscore, rscore\n",
    "\n",
    "print(f'{exceptional = }, {nominal = }, %exceptional = {exceptional / (exceptional + nominal) * 100 :.2f}%, %nominal = {nominal / (exceptional + nominal) * 100 :.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "csvs = os.listdir('./Data/ScoreInfo/Sabre/')\n",
    "\n",
    "nominal = 0\n",
    "exceptional = 0\n",
    "\n",
    "for csv in csvs:\n",
    "    info = pd.read_csv('./Data/ScoreInfo/Sabre/' + csv, header=0)\n",
    "    # info.set_index('frame_no')\n",
    "    info = info.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], errors='ignore')\n",
    "    info['frame_no']\n",
    "\n",
    "    csv_text = info.to_csv(index=False)\n",
    "    with open('./Data/ScoreInfo/Sabre/' + csv, 'w') as f:\n",
    "        f.write(csv_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cut clips based on labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-10.0\n",
      "30.0\n",
      "70.0\n",
      "110.0\n",
      "150.0\n",
      "190.0\n",
      "230.0\n",
      "270.00000000001455\n",
      "310.0\n",
      "350.0\n",
      "390.0\n",
      "430.0\n",
      "470.0\n",
      "510.0\n",
      "550.0\n",
      "590.0\n",
      "630.0000000000146\n",
      "670.0\n",
      "710.0\n",
      "750.0\n",
      "790.0\n",
      "830.0\n",
      "870.0\n",
      "910.0000000000146\n",
      "950.0\n",
      "990.0\n",
      "1030.0\n",
      "1070.0\n",
      "1110.0\n",
      "1150.0\n",
      "1190.0\n",
      "1230.0\n",
      "1270.0000000000146\n",
      "1310.0\n",
      "1350.0\n",
      "1390.0\n",
      "1430.0\n",
      "1470.0\n",
      "1510.0\n",
      "1550.0\n",
      "1590.0\n",
      "1630.0000000000146\n",
      "1670.0\n",
      "1710.0\n",
      "1750.0\n",
      "1790.0\n",
      "1830.0\n",
      "1870.0\n",
      "1910.0000000000146\n",
      "1950.0\n",
      "1990.0\n",
      "2030.0\n",
      "2070.0\n",
      "2110.0\n",
      "2150.0\n",
      "2190.0\n",
      "2230.0\n",
      "2270.0000000000146\n",
      "2310.0\n",
      "2350.0\n",
      "2390.0\n",
      "2430.0\n",
      "2470.0\n",
      "2510.0\n",
      "2550.0\n",
      "2590.0\n",
      "2630.0000000000146\n",
      "2670.0\n",
      "2710.0\n",
      "2750.0\n",
      "2790.0\n",
      "2830.0\n",
      "2870.0\n",
      "2910.0000000000146\n",
      "2950.0\n",
      "2990.0\n",
      "3030.0\n",
      "-10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "info = pd.read_csv('./Data/ScoreInfo/Sabre/0020sabre.csv', header=0)\n",
    "\n",
    "for i, row in info.iterrows():\n",
    "    # skip the \"first detection\" which is really just the 0-0 score in the first frame\n",
    "    if i == 0:\n",
    "        continue\n",
    "\n",
    "    frame_no, ms, lscore, rscore, lconf, rconf, nominal = row\n",
    "\n",
    "    # skip any data marked as not nominal\n",
    "    if not nominal:\n",
    "        continue\n",
    "\n",
    "    broken = False\n",
    "\n",
    "    # on \"average\" (not rigorously tested), referees change update the scoreboard ~2-3 seconds after the point is scored,\n",
    "    # so we can ignore the last second of video before a scoreboard update\n",
    "    clip_end = ms - 750\n",
    "\n",
    "    # using clip length of 5 seconds for now (maybe variable in the future?)\n",
    "    clip_start = clip_end - 3000\n",
    "\n",
    "    cap = cv2.VideoCapture('./Data/YtDownloads/Sabre/0020sabre.mp4')\n",
    "    cap.set(cv2.CAP_PROP_POS_MSEC, clip_start)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    while True:\n",
    "        if cap.get(cv2.CAP_PROP_POS_MSEC) > clip_end:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "\n",
    "        flags, frame = cap.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        clip_time = cap.get(cv2.CAP_PROP_POS_MSEC) - clip_start\n",
    "        cv2.imshow(f'frame', frame)\n",
    "        print(clip_time)\n",
    "\n",
    "        if cv2.waitKey(int(1000 / fps)) & 0xFF == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            broken = True\n",
    "            break\n",
    "\n",
    "        cv2.waitKey()\n",
    "\n",
    "    if broken:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./Data/YtDownloads/Sabre/0020sabre.mp4')\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, 5500)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 5375)\n",
    "# cap.set(cv2.CAP_PROP_POS_FRAMES, 5400)\n",
    "flags, frame = cap.read()\n",
    "\n",
    "cv2.imshow('frame', frame)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "# cv2.imwrite('./test/test_img.png', frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Isolating light bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left light is on!\n"
     ]
    }
   ],
   "source": [
    "PATCH_HEIGHT = 5\n",
    "PATCH_WIDTH = 100\n",
    "\n",
    "PATCH_TOP = 330\n",
    "PATCH_BOTTOM = PATCH_TOP + PATCH_HEIGHT\n",
    "L_PATCH_LEFT = 90\n",
    "L_PATCH_RIGHT = L_PATCH_LEFT + PATCH_WIDTH\n",
    "L_PATCH_MID = (L_PATCH_LEFT + L_PATCH_RIGHT) // 2 - 2\n",
    "R_PATCH_LEFT = 450\n",
    "R_PATCH_RIGHT = R_PATCH_LEFT + PATCH_WIDTH\n",
    "R_PATCH_MID = (R_PATCH_LEFT + R_PATCH_RIGHT) // 2 - 2\n",
    "ADJ = -32\n",
    "\n",
    "l_light = frame[PATCH_TOP:PATCH_BOTTOM, L_PATCH_LEFT:L_PATCH_RIGHT, :]\n",
    "cv2.imshow('left light', l_light)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "r_light = frame[PATCH_TOP:PATCH_BOTTOM, R_PATCH_LEFT:R_PATCH_RIGHT, :]\n",
    "cv2.imshow('right light', r_light)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "#encoded as BGR\n",
    "\n",
    "# left's red channel is high AND its green channel is low\n",
    "if (l_light[:, :, 2].sum() / l_light[:, :].size > 0.25 * 255) and l_light[:, :, 1].sum() / l_light[:, :].size < 0.25 * 255:\n",
    "    print('left light is on!')\n",
    "\n",
    "# right's green channel is high AND its red channel is low\n",
    "if (r_light[:, :, 1].sum() / r_light[:, :].size > 0.25 * 255) and r_light[:, :, 2].sum() / r_light[:, :].size < 0.25 * 255:\n",
    "    print('right light is on!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check clips after clipping\n",
    "video should turn gray after the light of the person who got the point turns on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "CSV_DIR = './Data/ScoreInfo/Sabre/'\n",
    "VID_DIR = './Data/YtDownloads/Sabre/'\n",
    "CLIP_DIR = './Data/Clips/Sabre/'\n",
    "\n",
    "csvs = os.listdir(CSV_DIR)\n",
    "broken = False\n",
    "\n",
    "# first, define a clip start and end in the csv for each score event\n",
    "for csv_file in csvs:\n",
    "    info = pd.read_csv(CSV_DIR + csv_file, header=0)\n",
    "\n",
    "    video_file = os.path.splitext(csv_file)[0] + '.mp4'\n",
    "\n",
    "    cap = cv2.VideoCapture(VID_DIR + video_file)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    for i, row in info.iterrows():\n",
    "        if i == 0 or not row['nominal']:\n",
    "            continue\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, row['clip_start_ms'])\n",
    "        while True:\n",
    "            flags, frame = cap.read()\n",
    "            if cap.get(cv2.CAP_PROP_POS_MSEC) > row['clip_end_ms']:\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            cv2.imshow(f'{video_file} - {row['lscore'], row['rscore']}', frame)\n",
    "\n",
    "            if cv2.waitKey(int(1000 / fps)) & 0xFF == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                broken = True\n",
    "                break\n",
    "\n",
    "            if cap.get(cv2.CAP_PROP_POS_MSEC) > row['clip_start_ms'] + 4000:\n",
    "                cv2.destroyAllWindows()\n",
    "                break\n",
    "\n",
    "\n",
    "        if broken:\n",
    "            break\n",
    "\n",
    "    if broken:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making sure everything was cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0371sabre.csv was not processed correctly!!!\n",
      "0086sabre.csv was not processed correctly!!!\n",
      "0159sabre.csv was not processed correctly!!!\n",
      "0624sabre.csv was not processed correctly!!!\n",
      "0362sabre.csv was not processed correctly!!!\n",
      "0131sabre.csv was not processed correctly!!!\n",
      "0268sabre.csv was not processed correctly!!!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "CSV_DIR = './Data/ScoreInfo/Sabre/'\n",
    "VID_DIR = './Data/YtDownloads/Sabre/'\n",
    "CLIP_DIR = './Data/Clips/Sabre/'\n",
    "\n",
    "csvs = os.listdir(CSV_DIR)\n",
    "\n",
    "# first, define a clip start and end in the csv for each score event\n",
    "for csv_file in csvs:\n",
    "    info = pd.read_csv(CSV_DIR + csv_file, header=0)\n",
    "    if 'clip_start_ms' not in info.columns:\n",
    "        print(f'{csv_file} was not processed correctly!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try marking keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow_docs.vis import embed\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Import matplotlib libraries\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.collections import LineCollection\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "# Some modules to display an animation using imageio.\n",
    "import imageio\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# Dictionary that maps from joint names to keypoint indices.\n",
    "KEYPOINT_DICT = {\n",
    "    'nose': 0,\n",
    "    'left_eye': 1,\n",
    "    'right_eye': 2,\n",
    "    'left_ear': 3,\n",
    "    'right_ear': 4,\n",
    "    'left_shoulder': 5,\n",
    "    'right_shoulder': 6,\n",
    "    'left_elbow': 7,\n",
    "    'right_elbow': 8,\n",
    "    'left_wrist': 9,\n",
    "    'right_wrist': 10,\n",
    "    'left_hip': 11,\n",
    "    'right_hip': 12,\n",
    "    'left_knee': 13,\n",
    "    'right_knee': 14,\n",
    "    'left_ankle': 15,\n",
    "    'right_ankle': 16\n",
    "}\n",
    "\n",
    "# Maps bones to a matplotlib color name.\n",
    "KEYPOINT_EDGE_INDS_TO_COLOR = {\n",
    "    (0, 1): 'm',\n",
    "    (0, 2): 'c',\n",
    "    (1, 3): 'm',\n",
    "    (2, 4): 'c',\n",
    "    (0, 5): 'm',\n",
    "    (0, 6): 'c',\n",
    "    (5, 7): 'm',\n",
    "    (7, 9): 'm',\n",
    "    (6, 8): 'c',\n",
    "    (8, 10): 'c',\n",
    "    (5, 6): 'y',\n",
    "    (5, 11): 'm',\n",
    "    (6, 12): 'c',\n",
    "    (11, 12): 'y',\n",
    "    (11, 13): 'm',\n",
    "    (13, 15): 'm',\n",
    "    (12, 14): 'c',\n",
    "    (14, 16): 'c'\n",
    "}\n",
    "\n",
    "def _keypoints_and_edges_for_display(keypoints_with_scores,\n",
    "                                     height,\n",
    "                                     width,\n",
    "                                     keypoint_threshold=0.11):\n",
    "  \"\"\"Returns high confidence keypoints and edges for visualization.\n",
    "\n",
    "  Args:\n",
    "    keypoints_with_scores: A numpy array with shape [n, p, 56] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model\n",
    "      * n: the batch number\n",
    "      * p: the person's index in the frame\n",
    "      * the last dimension is 17 * 3 + 5 for 17 keypoints, 3 components for each keypoint (x, y, conf(s)),\n",
    "        and [ymin, xmin, ymax, xmax, conf_score] for the bounding box\n",
    "    height: height of the image in pixels.\n",
    "    width: width of the image in pixels.\n",
    "    keypoint_threshold: minimum confidence score for a keypoint to be\n",
    "      visualized.\n",
    "\n",
    "  Returns:\n",
    "    A (keypoints_xy, edges_xy, edge_colors) containing:\n",
    "      * the coordinates of all keypoints of all detected entities;\n",
    "      * the coordinates of all skeleton edges of all detected entities;\n",
    "      * the colors in which the edges should be plotted.\n",
    "  \"\"\"\n",
    "  keypoints_all = []\n",
    "  keypoint_edges_all = []\n",
    "  edge_colors = []\n",
    "  keypoints_with_scores = keypoints_with_scores[:, :2, :51]\n",
    "  keypoints_with_scores = keypoints_with_scores.reshape(1, 2, 17, 3)\n",
    "  _, num_instances, _, _ = keypoints_with_scores.shape\n",
    "  for idx in range(num_instances):\n",
    "    kpts_x = keypoints_with_scores[0, idx, :, 1]\n",
    "    kpts_y = keypoints_with_scores[0, idx, :, 0]\n",
    "    kpts_scores = keypoints_with_scores[0, idx, :, 2]\n",
    "    kpts_absolute_xy = np.stack(\n",
    "        [width * np.array(kpts_x), height * np.array(kpts_y)], axis=-1)\n",
    "    kpts_above_thresh_absolute = kpts_absolute_xy[\n",
    "        kpts_scores > keypoint_threshold, :]\n",
    "    keypoints_all.append(kpts_above_thresh_absolute)\n",
    "\n",
    "    for edge_pair, color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n",
    "      if (kpts_scores[edge_pair[0]] > keypoint_threshold and\n",
    "          kpts_scores[edge_pair[1]] > keypoint_threshold):\n",
    "        x_start = kpts_absolute_xy[edge_pair[0], 0]\n",
    "        y_start = kpts_absolute_xy[edge_pair[0], 1]\n",
    "        x_end = kpts_absolute_xy[edge_pair[1], 0]\n",
    "        y_end = kpts_absolute_xy[edge_pair[1], 1]\n",
    "        line_seg = np.array([[x_start, y_start], [x_end, y_end]])\n",
    "        keypoint_edges_all.append(line_seg)\n",
    "        edge_colors.append(color)\n",
    "  if keypoints_all:\n",
    "    keypoints_xy = np.concatenate(keypoints_all, axis=0)\n",
    "  else:\n",
    "    keypoints_xy = np.zeros((0, 17, 2))\n",
    "\n",
    "  if keypoint_edges_all:\n",
    "    edges_xy = np.stack(keypoint_edges_all, axis=0)\n",
    "  else:\n",
    "    edges_xy = np.zeros((0, 2, 2))\n",
    "  return keypoints_xy, edges_xy, edge_colors\n",
    "\n",
    "\n",
    "def draw_prediction_on_image(\n",
    "    image, keypoints_with_scores, crop_region=None, close_figure=False,\n",
    "    output_image_height=None):\n",
    "  \"\"\"Draws the keypoint predictions on image.\n",
    "\n",
    "  Args:\n",
    "    image: A numpy array with shape [height, width, channel] representing the\n",
    "      pixel values of the input image.\n",
    "    keypoints_with_scores: A numpy array with shape [1, 1, 17, 3] representing\n",
    "      the keypoint coordinates and scores returned from the MoveNet model.\n",
    "    crop_region: A dictionary that defines the coordinates of the bounding box\n",
    "      of the crop region in normalized coordinates (see the init_crop_region\n",
    "      function below for more detail). If provided, this function will also\n",
    "      draw the bounding box on the image.\n",
    "    output_image_height: An integer indicating the height of the output image.\n",
    "      Note that the image aspect ratio will be the same as the input image.\n",
    "\n",
    "  Returns:\n",
    "    A numpy array with shape [out_height, out_width, channel] representing the\n",
    "    image overlaid with keypoint predictions.\n",
    "  \"\"\"\n",
    "  height, width, channel = image.shape\n",
    "  aspect_ratio = float(width) / height\n",
    "  fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n",
    "  # To remove the huge white borders\n",
    "  fig.tight_layout(pad=0)\n",
    "  ax.margins(0)\n",
    "  ax.set_yticklabels([])\n",
    "  ax.set_xticklabels([])\n",
    "  plt.axis('off')\n",
    "\n",
    "  im = ax.imshow(image)\n",
    "  line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n",
    "  ax.add_collection(line_segments)\n",
    "  # Turn off tick labels\n",
    "  scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n",
    "\n",
    "  (keypoint_locs, keypoint_edges,\n",
    "   edge_colors) = _keypoints_and_edges_for_display(\n",
    "       keypoints_with_scores, height, width)\n",
    "\n",
    "  line_segments.set_segments(keypoint_edges)\n",
    "  line_segments.set_color(edge_colors)\n",
    "  if keypoint_edges.shape[0]:\n",
    "    line_segments.set_segments(keypoint_edges)\n",
    "    line_segments.set_color(edge_colors)\n",
    "  if keypoint_locs.shape[0]:\n",
    "    scat.set_offsets(keypoint_locs)\n",
    "\n",
    "  if crop_region is not None:\n",
    "    xmin = max(crop_region['x_min'] * width, 0.0)\n",
    "    ymin = max(crop_region['y_min'] * height, 0.0)\n",
    "    rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n",
    "    rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n",
    "    rect = patches.Rectangle(\n",
    "        (xmin,ymin),rec_width,rec_height,\n",
    "        linewidth=1,edgecolor='b',facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "  fig.canvas.draw()\n",
    "  image_from_plot = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8).reshape(1200, 1200, -1)[:, :, :3]\n",
    "  image_from_plot = image_from_plot.reshape(\n",
    "      fig.canvas.get_width_height()[::-1] + (3,))\n",
    "  plt.close(fig)\n",
    "  if output_image_height is not None:\n",
    "    output_image_width = int(output_image_height / height * width)\n",
    "    image_from_plot = cv2.resize(\n",
    "        image_from_plot, dsize=(output_image_width, output_image_height),\n",
    "         interpolation=cv2.INTER_CUBIC)\n",
    "  return image_from_plot\n",
    "\n",
    "def to_gif(images, duration):\n",
    "  \"\"\"Converts image sequence (4D numpy array) to gif.\"\"\"\n",
    "  imageio.mimsave('./animation.gif', images, duration=duration)\n",
    "  return embed.embed_file('./animation.gif')\n",
    "\n",
    "def progress(value, max=100):\n",
    "  return HTML(\"\"\"\n",
    "      <progress\n",
    "          value='{value}'\n",
    "          max='{max}',\n",
    "          style='width: 100%'\n",
    "      >\n",
    "          {value}\n",
    "      </progress>\n",
    "  \"\"\".format(value=value, max=max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-05 16:15:22.363648: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as tfhub\n",
    "\n",
    "model = tfhub.load(\"https://www.kaggle.com/models/google/movenet/TensorFlow2/multipose-lightning/1\")\n",
    "movenet = model.signatures['serving_default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./Data/YtDownloads/Sabre/0020sabre.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 5400)\n",
    "while True:\n",
    "    flags, frame = cap.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    # cv2.imshow(f'frame', frame)\n",
    "\n",
    "    if cv2.waitKey(int(1000 / fps)) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        broken = True\n",
    "        break\n",
    "\n",
    "    flags, frame = cap.read()\n",
    "\n",
    "    input_image = tf.expand_dims(frame, axis=0)\n",
    "    input_image = tf.image.resize_with_pad(input_image, 128, 256)\n",
    "    input_image = tf.cast(input_image, tf.int32)\n",
    "\n",
    "    # Run model inference.\n",
    "    keypoints_with_scores = movenet(input_image)['output_0'].numpy()\n",
    "\n",
    "    # Visualize the predictions with image.\n",
    "    display_image = tf.expand_dims(frame, axis=0)\n",
    "    display_image = tf.cast(tf.image.resize_with_pad(\n",
    "        display_image, 1280, 1280), dtype=tf.int32)\n",
    "    output_overlay = draw_prediction_on_image(\n",
    "        np.squeeze(display_image.numpy(), axis=0), keypoints_with_scores)\n",
    "\n",
    "    # plt.figure(figsize=(5, 5))\n",
    "    # plt.imshow(output_overlay)\n",
    "    # _ = plt.axis('off')\n",
    "    cv2.imshow('frame', output_overlay)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
